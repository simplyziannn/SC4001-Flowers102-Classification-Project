{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f92cb3",
   "metadata": {},
   "source": [
    "## Flowers102 Hyperparameter Tuning and Model Comparison Overview\n",
    "\n",
    "This document, building upon the baseline and few-shot models presented earlier, specifically focuses on systematic hyperparameter tuning and result analysis for the Flowers102 classification task. It includes the following core objectives:\n",
    "- **Unified Training Interface**:\n",
    "    - Lightweight encapsulation of the original training function, enabling reuse of the same training and evaluation logic across different tuning strategies (greedy search / Optuna).\n",
    "- **Hyperparameter Search**:\n",
    "    - Using **staged greedy search (Greedy Tuning)**, adjusting the learning rate, weight decay, cosine classifier scaling factor, label smoothing factor, and batch size in a fixed order.\n",
    "    - Using **Optuna (TPE)** for joint hyperparameter optimization, automatically sampling and evaluating candidate configurations in both continuous and discrete spaces.\n",
    "- **Final Model Selection and Test Set Evaluation**:\n",
    "    - Automatically inferring and loading the corresponding optimal checkpoint based on the greedy or Optuna search results.\n",
    "    - Evaluate the performance difference between the \"best model after hyperparameter tuning\" and the \"baseline model\" on the TEST set, and save the comparison results as JSON for easy experiment reproduction and report writing.\n",
    "\n",
    "This document allows for experimentation and cross-sectional comparison of various hyperparameter tuning strategies while maintaining a consistent training code structure, providing a reliable basis for the final model selection in the Flowers102 project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f67e0",
   "metadata": {},
   "source": [
    "# Baseline Flowers102 pipeline setup\n",
    " - Initialize the baseline Flowers102 classification experiment using shared utilities from `flowers_common`.\n",
    " - Import helpers for seeding, device selection, data loading, model construction, training, evaluation, and inference.\n",
    " - Fix random seeds to ensure deterministic behavior across Python, NumPy, and PyTorch.\n",
    " - Select the compute device (CPU, CUDA, or MPS) via `get_device_config`.\n",
    " - Build deterministic train/validation/test `DataLoader` objects for the Flowers102 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eee3273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowers_common import seed_all, get_device_config, get_dataloaders\n",
    "from flowers_common import build_resnet50_cosine\n",
    "from flowers_common import train_model\n",
    "from flowers_common import build_resnet50_cosine\n",
    "\n",
    "seed_all(1029)\n",
    "dc = get_device_config()\n",
    "device = dc.device\n",
    "train_loader, val_loader, test_loader = get_dataloaders(root=\"data\", batch_size=32, img_size=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "LOAD_WEIGHTS = True\n",
    "CKPT_EXISTING = \"ckpt/resnet50_imagenet_finetuned_v1.pth\"\n",
    "CKPT_NEW      = \"ckpt/best_model_new.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0177a31e",
   "metadata": {},
   "source": [
    "## 3.1.1: Extended train_model guard for compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ea4c2",
   "metadata": {},
   "source": [
    "This section dynamically wraps an extended version of the imported train_model when it lacks the signature required for hyperparameter tuning:\n",
    "\n",
    "- Checks if the current train_model already includes parameters such as weight_decay using inspect.signature; if it meets the requirements, it returns directly.\n",
    "\n",
    "- If not, it defines train_model_extended as an alternative implementation, using a training loop of Adam + ReduceLROnPlateau + AMP.\n",
    "\n",
    "- Supports weight_decay and label_smoothing, and calculates training/validation set loss and accuracy at each epoch.\n",
    "\n",
    "- Uses an early stopping strategy and validation set performance (loss + accuracy) to track the optimal model, saving the best checkpoint as performance improves.\n",
    "\n",
    "- Records the metrics and trial_id for each epoch to an external log via the epoch_log_cb callback for subsequent hyperparameter tuning analysis.\n",
    "\n",
    "Finally, the global train_model is set to train_model_extended, achieving backward compatibility with the old public training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Callable, Optional, Dict, Any\n",
    "\n",
    "\n",
    "def _extended_train_model_guard():\n",
    "    import inspect\n",
    "    sig = inspect.signature(train_model)\n",
    "    if \"weight_decay\" in sig.parameters:\n",
    "        return\n",
    "    orig_train_model = train_model\n",
    "\n",
    "    def train_model_extended(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        epochs=30,\n",
    "        lr=1e-4,\n",
    "        patience=5,\n",
    "        ckpt_path=\"ckpt/best.pth\",\n",
    "        *,\n",
    "        weight_decay: float = 0.0,\n",
    "        label_smoothing: float = 0.0,\n",
    "        epoch_log_cb: Optional[Callable[[Dict[str, Any]], None]] = None,\n",
    "        trial_id: Optional[str] = None,\n",
    "    ):\n",
    "        import os, time\n",
    "        from torch.optim import Adam\n",
    "        from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "        from torch.cuda.amp import autocast, GradScaler\n",
    "        import torch.nn as nn\n",
    "        import torch\n",
    "\n",
    "        os.makedirs(\"ckpt\", exist_ok=True)\n",
    "        model = model.to(device)\n",
    "\n",
    "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "        scaler = GradScaler(enabled=True)\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_epoch = -1\n",
    "        best_val_acc = -1.0\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # ---- Train ----\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            correct, total = 0, 0\n",
    "            t0 = time.time()\n",
    "\n",
    "            for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\"):\n",
    "                imgs = imgs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=True):\n",
    "                    logits = model(imgs)\n",
    "                    loss = criterion(logits, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                preds = logits.argmax(1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            train_loss = total_loss / max(1, len(train_loader))\n",
    "            train_acc = correct / max(1, total)\n",
    "\n",
    "            # ---- Val ----\n",
    "            model.eval()\n",
    "            val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for imgs, labels in val_loader:\n",
    "                    imgs = imgs.to(device, non_blocking=True)\n",
    "                    labels = labels.to(device, non_blocking=True)\n",
    "                    with autocast(enabled=True):\n",
    "                        logits = model(imgs)\n",
    "                        loss = criterion(logits, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    preds = logits.argmax(1)\n",
    "                    val_correct += (preds == labels).sum().item()\n",
    "                    val_total += labels.size(0)\n",
    "\n",
    "            val_loss /= max(1, len(val_loader))\n",
    "            val_acc = val_correct / max(1, val_total)\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            # ---- Early stopping & best tracking ----\n",
    "            improved = val_loss < best_val_loss or (math.isclose(val_loss, best_val_loss, rel_tol=0.0, abs_tol=1e-12) and val_acc > best_val_acc)\n",
    "            if improved:\n",
    "                best_val_loss = val_loss\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = epoch\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), ckpt_path)\n",
    "                print(f\"Saved best model to {ckpt_path}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\" EarlyStopping counter {patience_counter}/{patience}\")\n",
    "                if patience_counter >= patience:\n",
    "                    print(\" Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "            # ---- Optional epoch log callback ----\n",
    "            if epoch_log_cb is not None:\n",
    "                epoch_log_cb({\n",
    "                    \"trial_id\": trial_id,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"train_acc\": train_acc,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"val_acc\": val_acc,\n",
    "                    \"lr\": float(optimizer.param_groups[0][\"lr\"]),\n",
    "                    \"elapsed_sec\": time.time() - t0,\n",
    "                })\n",
    "\n",
    "        return {\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"best_val_loss\": float(best_val_loss),\n",
    "            \"best_val_acc\": float(best_val_acc),\n",
    "            \"ckpt_path\": ckpt_path,\n",
    "        }\n",
    "\n",
    "    globals()[\"train_model\"] = train_model_extended\n",
    "\n",
    "_extended_train_model_guard()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f805cc",
   "metadata": {},
   "source": [
    "## 3.1.2: Tuning utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09899593",
   "metadata": {},
   "source": [
    "This section implements reusable tools for the hyperparameter tuning phase:\n",
    "\n",
    "- JSONLLogger: Appends training metrics for each epoch/trial to a JSON Lines log file for easy visualization and analysis later.\n",
    "\n",
    "- set_cosine_scale: Sets the scaling factor 's' for CosineClassifier uniformly, ensuring consistent decision boundaries across different experiments.\n",
    "\n",
    "- build_model_with_safe_load: Rebuilds the ResNet50+CosineClassifier model and safely loads backbone/FC parameters when fine-tuning weights are available.\n",
    "\n",
    "- ensure_included: Ensures baseline configurations are included when constructing the hyperparameter search space, facilitating direct performance comparison with \"untuned\" models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c71cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.1.2. Tuning utilities\n",
    "# =============================================================================\n",
    "\n",
    "class JSONLLogger:\n",
    "    \"\"\"Append JSON objects per line for later analysis.\"\"\"\n",
    "    def __init__(self, path: str):\n",
    "        self.path = path\n",
    "        os.makedirs(os.path.dirname(self.path), exist_ok=True)\n",
    "    def log(self, obj: Dict[str, Any]):\n",
    "        with open(self.path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def set_cosine_scale(model: nn.Module, s: float) -> None:\n",
    "    \"\"\"Set the CosineClassifier scale if present.\"\"\"\n",
    "    try:\n",
    "        model[1].s = float(s)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def build_model_with_safe_load() -> nn.Module:\n",
    "    \"\"\"Rebuild a fresh model and reuse the same safe-load logic as baseline.\"\"\"\n",
    "    m = build_resnet50_cosine(num_classes=102, pretrained=True)\n",
    "    # Reuse the transplant logic if the file exists.\n",
    "    if 'LOAD_WEIGHTS' in globals() and LOAD_WEIGHTS and 'CKPT_EXISTING' in globals() and os.path.exists(CKPT_EXISTING):\n",
    "        try:\n",
    "            state = torch.load(CKPT_EXISTING, map_location=device)\n",
    "            if isinstance(state, dict) and \"state_dict\" in state:\n",
    "                state = state[\"state_dict\"]\n",
    "            m[0].load_state_dict(state, strict=False)\n",
    "            # Attempt FC to cosine transplant\n",
    "            def first_present(d, keys):\n",
    "                for k in keys:\n",
    "                    if k in d:\n",
    "                        return k\n",
    "                return None\n",
    "            w_key = first_present(state, [\"fc.1.weight\", \"fc.weight\"])\n",
    "            W = state[w_key] if w_key else None\n",
    "            if W is not None and W.shape == m[1].weight.shape:\n",
    "                with torch.no_grad():\n",
    "                    m[1].weight.copy_(F.normalize(W, dim=1))\n",
    "                    m[1].s = 30.0\n",
    "        except Exception:\n",
    "            pass\n",
    "    return m\n",
    "\n",
    "def ensure_included(seq, base):\n",
    "    \"\"\"Ensure baseline value is included while preserving readable order.\"\"\"\n",
    "    if base in seq:\n",
    "        return list(seq)\n",
    "    return [base] + list(seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d51c41b",
   "metadata": {},
   "source": [
    "## 3.1.3: Greedy tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5553a7",
   "metadata": {},
   "source": [
    "This section implements a phased greedy hyperparameter searcher:\n",
    "\n",
    "- The search order is fixed: learning rate (lr) → weight decay (weight_decay) → cosine head scaling (s) → label smoothing (label_smoothing) → batch_size.\n",
    "\n",
    "- Each phase, based on the optimal configuration of the previous phase, only changes the hyperparameters to be tuned, training on candidate values ​​one by one and evaluating on the validation set.\n",
    "\n",
    "- The evaluation metric is primarily validation set accuracy; when accuracies are equal, the lower validation set loss is prioritized.\n",
    "\n",
    "- The training process is written to logs/train_process.jsonl via JSONLLogger, and the summary results of each trial are written to logs/greedy_results.json.\n",
    "\n",
    "- This module ultimately provides a set of \"optimal hyperparameter configurations\" obtained through the greedy search, providing input for subsequent visualization and testing evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.1.3. Greedy tuner\n",
    "# =============================================================================\n",
    "def greedy_tune():\n",
    "    \"\"\"\n",
    "    Greedy search over discrete grids in fixed order:\n",
    "    lr -> weight_decay -> s -> label_smoothing -> batch_size.\n",
    "    Use validation accuracy as primary metric and validation loss as tie-breaker.\n",
    "    Output files:\n",
    "      - logs/train_process.jsonl : per-epoch records across trials\n",
    "      - logs/greedy_results.json : trial summaries and final best config\n",
    "    \"\"\"\n",
    "    # 1) Define search spaces (baseline included)\n",
    "    BASELINE = {\"lr\": 1e-4, \"weight_decay\": 0.0, \"s\": 30.0, \"label_smoothing\": 0.0, \"batch_size\": 32}\n",
    "    SPACE = {\n",
    "        \"lr\":               ensure_included([3e-5, 2e-4, 3e-4, 5e-4], BASELINE[\"lr\"]),\n",
    "        \"weight_decay\":     ensure_included([1e-6, 1e-5, 5e-5, 1e-4, 3e-4, 0], BASELINE[\"weight_decay\"]),\n",
    "        \"s\":                ensure_included([10, 16, 24, 40, 64], BASELINE[\"s\"]),\n",
    "        \"label_smoothing\":  ensure_included([0.05, 0.1, 0.15], BASELINE[\"label_smoothing\"]),\n",
    "        \"batch_size\":       ensure_included([16, 48, 64], BASELINE[\"batch_size\"]),\n",
    "    }\n",
    "    ORDER = [\"lr\", \"weight_decay\", \"s\", \"label_smoothing\", \"batch_size\"]\n",
    "\n",
    "    # 2) Common training knobs\n",
    "    EPOCHS = 30\n",
    "    PATIENCE = 5\n",
    "    IMG_SIZE = 224\n",
    "    NUM_WORKERS = 0\n",
    "    PIN_MEMORY = False\n",
    "    AUGMENT = True\n",
    "    SEED = 1029\n",
    "\n",
    "    # 3) Initialize loggers\n",
    "    process_logger = JSONLLogger(\"logs/train_process.jsonl\")\n",
    "    results: Dict[str, Any] = {\n",
    "        \"baseline\": BASELINE,\n",
    "        \"order\": ORDER,\n",
    "        \"search_space\": SPACE,\n",
    "        \"trials\": [],\n",
    "    }\n",
    "\n",
    "    # 4) Current best config (start with baseline)\n",
    "    best_cfg = dict(BASELINE)\n",
    "    best_of_stage: Dict[str, Any] = {}\n",
    "\n",
    "    # Prepare initial loaders; reuse when batch size is unchanged\n",
    "    train_loader_g, val_loader_g, test_loader_g = get_dataloaders(\n",
    "        batch_size=best_cfg[\"batch_size\"], img_size=IMG_SIZE,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "        augment=AUGMENT, seed=SEED\n",
    "    )\n",
    "\n",
    "    def build_loaders_for(bs: int):\n",
    "        nonlocal train_loader_g, val_loader_g, test_loader_g\n",
    "        if bs == train_loader_g.batch_size:\n",
    "            return train_loader_g, val_loader_g, test_loader_g\n",
    "        # Rebuild deterministically for a new batch size\n",
    "        return get_dataloaders(\n",
    "            batch_size=bs, img_size=IMG_SIZE,\n",
    "            num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "            augment=AUGMENT, seed=SEED\n",
    "        )\n",
    "\n",
    "    trial_counter = 0\n",
    "\n",
    "    for stage_param in ORDER:\n",
    "        stage_best = {\"val_acc\": -1.0, \"val_loss\": float(\"inf\"), \"value\": None, \"summary\": None}\n",
    "        print(f\"\\n[GREEDY] Stage: {stage_param} | Candidates: {SPACE[stage_param]}\")\n",
    "        for cand in SPACE[stage_param]:\n",
    "            cfg = dict(best_cfg)\n",
    "            cfg[stage_param] = cand\n",
    "            trial_counter += 1\n",
    "            trial_id = f\"stage_{stage_param}_trial_{trial_counter}_val_{str(cand)}\"\n",
    "\n",
    "            # Build loaders (only if batch size changes)\n",
    "            try:\n",
    "                train_loader, val_loader, _ = build_loaders_for(cfg[\"batch_size\"])\n",
    "            except RuntimeError as e:\n",
    "                # e.g., DataLoader worker init issues; record and continue\n",
    "                results[\"trials\"].append({\n",
    "                    \"trial_id\": trial_id, \"status\": \"failed\", \"reason\": str(e), \"config\": cfg\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # Fresh model per trial; same initialization and weight-loading policy as baseline\n",
    "            model_t = build_model_with_safe_load()\n",
    "            set_cosine_scale(model_t, cfg[\"s\"])\n",
    "\n",
    "            ckpt_dir = os.path.join(\"ckpt\", \"greedy\", stage_param, str(cand))\n",
    "            os.makedirs(ckpt_dir, exist_ok=True)\n",
    "            ckpt_path = os.path.join(ckpt_dir, \"best.pth\")\n",
    "\n",
    "            print(f\"[GREEDY] Trial {trial_id} | cfg={cfg}\")\n",
    "            status = \"ok\"\n",
    "            summary = None\n",
    "            try:\n",
    "                summary = train_model(\n",
    "                    model_t,\n",
    "                    train_loader,\n",
    "                    val_loader,\n",
    "                    epochs=EPOCHS,\n",
    "                    lr=cfg[\"lr\"],\n",
    "                    patience=PATIENCE,\n",
    "                    ckpt_path=ckpt_path,\n",
    "                    weight_decay=cfg[\"weight_decay\"],\n",
    "                    label_smoothing=cfg[\"label_smoothing\"],\n",
    "                    epoch_log_cb=lambda row: process_logger.log({\n",
    "                        **row,\n",
    "                        \"param\": stage_param,\n",
    "                        \"value\": cand,\n",
    "                        \"config\": cfg,\n",
    "                    }),\n",
    "                    trial_id=trial_id,\n",
    "                )\n",
    "                # Track stage best: prioritize higher val_acc; tie-break on lower val_loss\n",
    "                cur_va, cur_vl = summary[\"best_val_acc\"], summary[\"best_val_loss\"]\n",
    "                better = (cur_va > stage_best[\"val_acc\"]) or (\n",
    "                    math.isclose(cur_va, stage_best[\"val_acc\"], rel_tol=0.0, abs_tol=1e-12) and cur_vl < stage_best[\"val_loss\"]\n",
    "                )\n",
    "                if better:\n",
    "                    stage_best.update({\"val_acc\": cur_va, \"val_loss\": cur_vl, \"value\": cand, \"summary\": summary})\n",
    "                results[\"trials\"].append({\n",
    "                    \"trial_id\": trial_id, \"status\": status, \"config\": cfg,\n",
    "                    \"best_epoch\": summary[\"best_epoch\"],\n",
    "                    \"best_val_acc\": summary[\"best_val_acc\"],\n",
    "                    \"best_val_loss\": summary[\"best_val_loss\"],\n",
    "                    \"ckpt_path\": summary[\"ckpt_path\"],\n",
    "                })\n",
    "            except RuntimeError as e:\n",
    "                # Handle OOM gracefully for large batch sizes, etc.\n",
    "                status = \"failed_oom\" if \"out of memory\" in str(e).lower() else \"failed\"\n",
    "                print(f\"[WARN] Trial {trial_id} failed: {e}\")\n",
    "                results[\"trials\"].append({\n",
    "                    \"trial_id\": trial_id, \"status\": status, \"reason\": str(e), \"config\": cfg\n",
    "                })\n",
    "            finally:\n",
    "                # Free CUDA memory between trials\n",
    "                try:\n",
    "                    del model_t\n",
    "                    torch.cuda.empty_cache()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        # Commit stage winner\n",
    "        if stage_best[\"value\"] is not None:\n",
    "            best_cfg[stage_param] = stage_best[\"value\"]\n",
    "            best_of_stage[stage_param] = stage_best\n",
    "        print(f\"[GREEDY] Stage {stage_param} best: {stage_best['value']} \"\n",
    "              f\"(val_acc={stage_best['val_acc']:.4f}, val_loss={stage_best['val_loss']:.4f})\")\n",
    "\n",
    "    results[\"best_config\"] = best_cfg\n",
    "    results[\"stage_best\"] = best_of_stage\n",
    "\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    with open(\"logs/greedy_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"\\n[GREEDY] Done.\")\n",
    "    print(f\"[GREEDY] Final best config: {best_cfg}\")\n",
    "    print('[GREEDY] Files written: \"logs/train_process.jsonl\", \"logs/greedy_results.json\"')\n",
    "\n",
    "\n",
    "RUN_GREEDY = True  # set to False to disable\n",
    "if RUN_GREEDY:\n",
    "    greedy_tune()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3984b99",
   "metadata": {},
   "source": [
    "## 3.1.4: Optuna-based Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd41ac2e",
   "metadata": {},
   "source": [
    "This section uses Optuna for joint hyperparameter optimization as a supplement to the greedy search:\n",
    "\n",
    "- Introducing Optuna and TPESampler to automatically sample hyperparameter combinations in continuous/discrete spaces.\n",
    "\n",
    "- `run_optuna_tuning`:\n",
    "\n",
    "    - Defines the same or compatible hyperparameter search space as the greedy search and explicitly includes baseline configurations for fair comparison.\n",
    "\n",
    "    - Rebuilds the model and data loader for each trial, trains using an extended version of `train_model`, and logs the metrics for each epoch to `logs/optuna_train_process.json`.\n",
    "\n",
    "    - Writes the best epoch, validation set performance, and ckpt path for each trial to `logs/optuna_results.json`.\n",
    "\n",
    "    - This section shares the same log format as the previous greedy hyperparameter tuning for unified visualization and post-processing.\n",
    "\n",
    "    - Controls whether the HPO is actually run via additional entry switches (such as `RUN_OPTUNA_TUNING` or the `main` function) to avoid unnecessary long searches in the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41726cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.1.4. Optuna-based hyperparameter optimization\n",
    "# =============================================================================\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "def run_optuna_tuning():\n",
    "    \"\"\"\n",
    "    Joint HPO with Optuna (TPE). Logging schema matches the greedy version:\n",
    "      - Per-epoch: logs/optuna_train_process.jsonl (JSON Lines)\n",
    "        Keys: trial_id, epoch, train_loss, train_acc, val_loss, val_acc, lr, elapsed_sec,\n",
    "               and for schema parity: param='optuna', value=None, config=dict\n",
    "      - Per-trial summary: logs/optuna_results.json (JSON)\n",
    "        Keys: trial_id, status, config, best_epoch, best_val_acc, best_val_loss, ckpt_path\n",
    "    \"\"\"\n",
    "    assert optuna is not None, \"Optuna not installed.\"\n",
    "\n",
    "    # ---- (1) Search space with baseline included ----\n",
    "    BASELINE = {\"lr\": 1e-4, \"weight_decay\": 0.0, \"s\": 30.0, \"label_smoothing\": 0.0, \"batch_size\": 32}\n",
    "    SPACE = {\n",
    "        \"lr\":               {\"type\": \"log_float\", \"low\": 3e-5, \"high\": 5e-4},\n",
    "        \"weight_decay\":     {\"type\": \"categorical\", \"choices\": [0.0, 1e-6, 1e-5, 5e-5, 1e-4, 3e-4]},\n",
    "        \"s\":                {\"type\": \"categorical\", \"choices\": [10, 16, 24, 30, 40, 64]},\n",
    "        \"label_smoothing\":  {\"type\": \"categorical\", \"choices\": [0.0, 0.05, 0.1, 0.15]},\n",
    "        \"batch_size\":       {\"type\": \"categorical\", \"choices\": [16, 32, 48, 64]},\n",
    "    }\n",
    "\n",
    "    # ---- (2) Common knobs: keep identical to greedy search ----\n",
    "    EPOCHS = 30\n",
    "    PATIENCE = 5\n",
    "    IMG_SIZE = 224\n",
    "    NUM_WORKERS = 0\n",
    "    PIN_MEMORY = False\n",
    "    AUGMENT = True\n",
    "    SEED = 1029\n",
    "\n",
    "    # ---- (3) Loggers and results collector ----\n",
    "    process_logger = JSONLLogger(\"logs/optuna_train_process.jsonl\")\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    results = {\n",
    "        \"baseline\": BASELINE,\n",
    "        \"search_space\": SPACE,\n",
    "        \"sampler\": \"TPE\",\n",
    "        \"order\": [\"joint\"],\n",
    "        \"trials\": [],\n",
    "    }\n",
    "\n",
    "    # ---- (4) Objective definition ----\n",
    "    def objective(trial: \"optuna.trial.Trial\") -> float: # type: ignore\n",
    "        # Sample params (include baseline later via enqueue_trial)\n",
    "        if SPACE[\"lr\"][\"type\"] == \"log_float\":\n",
    "            lr = trial.suggest_float(\"lr\", SPACE[\"lr\"][\"low\"], SPACE[\"lr\"][\"high\"], log=True)\n",
    "        else:\n",
    "            lr = trial.suggest_float(\"lr\", SPACE[\"lr\"][\"low\"], SPACE[\"lr\"][\"high\"])\n",
    "\n",
    "        weight_decay = trial.suggest_categorical(\"weight_decay\", SPACE[\"weight_decay\"][\"choices\"])\n",
    "        s = trial.suggest_categorical(\"s\", SPACE[\"s\"][\"choices\"])\n",
    "        label_smoothing = trial.suggest_categorical(\"label_smoothing\", SPACE[\"label_smoothing\"][\"choices\"])\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", SPACE[\"batch_size\"][\"choices\"])\n",
    "\n",
    "        cfg = {\n",
    "            \"lr\": float(lr),\n",
    "            \"weight_decay\": float(weight_decay),\n",
    "            \"s\": float(s),\n",
    "            \"label_smoothing\": float(label_smoothing),\n",
    "            \"batch_size\": int(batch_size),\n",
    "        }\n",
    "\n",
    "        # Data loaders (deterministic build)\n",
    "        try:\n",
    "            train_loader, val_loader, _ = get_dataloaders(\n",
    "                batch_size=cfg[\"batch_size\"], img_size=IMG_SIZE,\n",
    "                num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "                augment=AUGMENT, seed=SEED\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            # Worker init or other loader errors; mark failed trial\n",
    "            results[\"trials\"].append({\n",
    "                \"trial_id\": f\"optuna_trial_{trial.number}\",\n",
    "                \"status\": \"failed\",\n",
    "                \"reason\": str(e),\n",
    "                \"config\": cfg\n",
    "            })\n",
    "            return 0.0\n",
    "\n",
    "        # Fresh model per trial; reuse baseline safe-load/transplant policy\n",
    "        model_t = build_model_with_safe_load()\n",
    "        set_cosine_scale(model_t, cfg[\"s\"])\n",
    "\n",
    "        ckpt_dir = os.path.join(\"ckpt\", \"optuna\", f\"trial_{trial.number}\")\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "        ckpt_path = os.path.join(ckpt_dir, \"best.pth\")\n",
    "\n",
    "        trial_id = f\"optuna_trial_{trial.number}\"\n",
    "\n",
    "        # Per-epoch logger to match schema; no mid-epoch pruning to keep training identical\n",
    "        def _epoch_cb(row: Dict[str, Any]):\n",
    "            # Keep keys consistent with greedy logger\n",
    "            process_logger.log({\n",
    "                **row,\n",
    "                \"param\": \"optuna\",\n",
    "                \"value\": None,\n",
    "                \"config\": cfg,\n",
    "            })\n",
    "            # Optional: report to Optuna for visualization; no pruning to keep parity\n",
    "            try:\n",
    "                trial.report(float(row[\"val_acc\"]), step=int(row[\"epoch\"]))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        status = \"ok\"\n",
    "        try:\n",
    "            summary = train_model(\n",
    "                model_t,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                epochs=EPOCHS,\n",
    "                lr=cfg[\"lr\"],\n",
    "                patience=PATIENCE,\n",
    "                ckpt_path=ckpt_path,\n",
    "                weight_decay=cfg[\"weight_decay\"],\n",
    "                label_smoothing=cfg[\"label_smoothing\"],\n",
    "                epoch_log_cb=_epoch_cb,\n",
    "                trial_id=trial_id,\n",
    "            )\n",
    "            results[\"trials\"].append({\n",
    "                \"trial_id\": trial_id,\n",
    "                \"status\": status,\n",
    "                \"config\": cfg,\n",
    "                \"best_epoch\": summary[\"best_epoch\"],\n",
    "                \"best_val_acc\": float(summary[\"best_val_acc\"]),\n",
    "                \"best_val_loss\": float(summary[\"best_val_loss\"]),\n",
    "                \"ckpt_path\": summary[\"ckpt_path\"],\n",
    "            })\n",
    "            return float(summary[\"best_val_acc\"])\n",
    "        except RuntimeError as e:\n",
    "            status = \"failed_oom\" if \"out of memory\" in str(e).lower() else \"failed\"\n",
    "            print(f\"[WARN] Trial {trial_id} failed: {e}\")\n",
    "            results[\"trials\"].append({\n",
    "                \"trial_id\": trial_id,\n",
    "                \"status\": status,\n",
    "                \"reason\": str(e),\n",
    "                \"config\": cfg\n",
    "            })\n",
    "            return 0.0\n",
    "        finally:\n",
    "            try:\n",
    "                del model_t\n",
    "                torch.cuda.empty_cache()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # ---- (5) Create study and run ----\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=SEED))\n",
    "    # Ensure baseline is evaluated\n",
    "    study.enqueue_trial(BASELINE)\n",
    "\n",
    "    OPTUNA_TRIALS = 16  # adjust as needed; includes the enqueued baseline\n",
    "    study.optimize(objective, n_trials=OPTUNA_TRIALS, n_jobs=1)\n",
    "\n",
    "    # ---- (6) Persist study-level results with same schema keys ----\n",
    "    results.update({\n",
    "        \"study_best_value\": float(study.best_value) if study.best_trial is not None else None,\n",
    "        \"best_config\": dict(study.best_params) if study.best_trial is not None else None,\n",
    "        \"n_trials\": len(study.trials),\n",
    "    })\n",
    "    with open(\"logs/optuna_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"\\n[OPTUNA] Done.\")\n",
    "    if study.best_trial is not None:\n",
    "        print(f\"[OPTUNA] Best val_acc={study.best_value:.4f} with params={study.best_params}\")\n",
    "    print('[OPTUNA] Files written: \"logs/optuna_train_process.jsonl\", \"logs/optuna_results.json\"')\n",
    "\n",
    "# Entry point switch\n",
    "RUN_OPTUNA = True  # set to False to disable Optuna run\n",
    "if optuna is not None and RUN_OPTUNA:\n",
    "    run_optuna_tuning()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b310187",
   "metadata": {},
   "source": [
    "## 3.1.5: Display best configurations and compare on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd3045b",
   "metadata": {},
   "source": [
    "This section summarizes and compares the final performance of the baseline, greedy, and optuna configurations on the TEST set:\n",
    "\n",
    "- Use `_safe_load_json` to read `logs/greedy_results.json` and `logs/optuna_results.json` to obtain the best configuration and summary information for each trial;\n",
    "\n",
    "- Use `_cfg_equal` and `_find_trial_ckpt` to find checkpoints matching the target configuration in the results. If a strict match fails, fall back to the trial with the highest validation accuracy;\n",
    "\n",
    "- `_load_model_from_ckpt` reconstructs the model based on the configuration, sets the scaling factor `s` of the `CosineClassifier`, and loads the corresponding checkpoint weights;\n",
    "\n",
    "- `_build_test_loader` reuses the construction logic of Flowers102, constructing a TEST DataLoader according to the given batch size, maintaining the same preprocessing strategy as the training/parameter tuning phase;\n",
    "\n",
    "- `_evaluate_on_test` calculates the accuracy and... macro/weighted precision/recall/F1 is used for quantitative comparison of model performance;\n",
    "\n",
    "- compare_best_and_baseline evaluates baseline, greedy_best, and optuna_best sequentially, prints key results, and writes the complete comparison to logs/final_compare.json;\n",
    "\n",
    "- RUN_FINAL_COMPARE acts as a switch to control whether this final comparison process is automatically executed when the script or Notebook starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee4728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.1.5. Display best configs and compare on test set\n",
    "# =============================================================================\n",
    "# This block reads best configs from greedy/optuna result files, loads their checkpoints,\n",
    "# evaluates on the test set, compares with baseline, and writes logs/final_compare.json.\n",
    "\n",
    "from typing import Dict, Any, Optional\n",
    "from copy import deepcopy\n",
    "\n",
    "def _safe_load_json(path: str) -> Optional[Dict[str, Any]]:\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[INFO] File not found: {path}\")\n",
    "        return None\n",
    "\n",
    "def _cfg_equal(a: Dict[str, Any], b: Dict[str, Any], eps: float = 1e-8) -> bool:\n",
    "    \"\"\"Float-tolerant equality for configs.\"\"\"\n",
    "    if a.keys() != b.keys():\n",
    "        return False\n",
    "    for k in a:\n",
    "        va, vb = a[k], b[k]\n",
    "        if isinstance(va, float) or isinstance(vb, float):\n",
    "            if not (abs(float(va) - float(vb)) <= eps):\n",
    "                return False\n",
    "        else:\n",
    "            if va != vb:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def _find_trial_ckpt(results_json: Dict[str, Any], target_cfg: Dict[str, Any]) -> Optional[str]:\n",
    "    \"\"\"Find ckpt_path for the exact target config in a results JSON (greedy/optuna).\"\"\"\n",
    "    trials = results_json.get(\"trials\", [])\n",
    "    for t in trials:\n",
    "        cfg = t.get(\"config\")\n",
    "        if not cfg or t.get(\"status\") not in (\"ok\",):\n",
    "            continue\n",
    "        if _cfg_equal(cfg, target_cfg):\n",
    "            return t.get(\"ckpt_path\")\n",
    "    # Fallback: best across trials, if exact match not found\n",
    "    best = None\n",
    "    for t in trials:\n",
    "        if t.get(\"status\") != \"ok\":\n",
    "            continue\n",
    "        va = float(t.get(\"best_val_acc\", 0.0))\n",
    "        if best is None or va > best[0]:\n",
    "            best = (va, t.get(\"ckpt_path\"))\n",
    "    return best[1] if best else None\n",
    "\n",
    "def _load_model_from_ckpt(cfg: Dict[str, Any], ckpt_path: str):\n",
    "    \"\"\"Rebuild model, set cosine scale, and load weights.\"\"\"\n",
    "    m = build_model_with_safe_load()\n",
    "    set_cosine_scale(m, float(cfg.get(\"s\", 30.0)))\n",
    "    assert os.path.exists(ckpt_path), f\"Checkpoint not found: {ckpt_path}\"\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    m.load_state_dict(state, strict=True)\n",
    "    return m.to(device)\n",
    "\n",
    "def _evaluate_on_test(model, test_loader) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate on test set and return accuracy + macro/weighted averages.\"\"\"\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(test_loader, desc=\"Testing(best)\"):\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            logits = model(imgs)\n",
    "            preds = logits.argmax(1).detach().cpu().numpy().tolist()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "    acc = float(accuracy_score(all_labels, all_preds))\n",
    "    rep = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_avg\": {\n",
    "            \"precision\": float(rep.get(\"macro avg\", {}).get(\"precision\", 0.0)),\n",
    "            \"recall\": float(rep.get(\"macro avg\", {}).get(\"recall\", 0.0)),\n",
    "            \"f1\": float(rep.get(\"macro avg\", {}).get(\"f1-score\", 0.0)),\n",
    "        },\n",
    "        \"weighted_avg\": {\n",
    "            \"precision\": float(rep.get(\"weighted avg\", {}).get(\"precision\", 0.0)),\n",
    "            \"recall\": float(rep.get(\"weighted avg\", {}).get(\"recall\", 0.0)),\n",
    "            \"f1\": float(rep.get(\"weighted avg\", {}).get(\"f1-score\", 0.0)),\n",
    "        },\n",
    "    }\n",
    "\n",
    "def _build_test_loader(batch_size: int):\n",
    "    \"\"\"Reuse the deterministic loader builder to get the test loader.\"\"\"\n",
    "    _, _, test_loader = get_dataloaders(\n",
    "        batch_size=batch_size,\n",
    "        img_size=224,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        augment=True,   # keep identical preprocessing policy\n",
    "        seed=1029\n",
    "    )\n",
    "    return test_loader\n",
    "\n",
    "def compare_best_and_baseline():\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    out = {\"baseline\": None, \"greedy_best\": None, \"optuna_best\": None}\n",
    "\n",
    "    # ---- Baseline config and checkpoint ----\n",
    "    baseline_cfg = {\"lr\": 1e-4, \"weight_decay\": 0.0, \"s\": 30.0, \"label_smoothing\": 0.0, \"batch_size\": 32}\n",
    "    baseline_ckpt = \"ckpt/best_cosine_cuda.pth\"  # set earlier in baseline training\n",
    "    if os.path.exists(baseline_ckpt):\n",
    "        try:\n",
    "            test_loader = _build_test_loader(baseline_cfg[\"batch_size\"])\n",
    "            model_b = build_model_with_safe_load()\n",
    "            set_cosine_scale(model_b, baseline_cfg[\"s\"])\n",
    "            state = torch.load(baseline_ckpt, map_location=device)\n",
    "            model_b.load_state_dict(state, strict=True)\n",
    "            metrics_b = _evaluate_on_test(model_b, test_loader)\n",
    "            out[\"baseline\"] = {\n",
    "                \"config\": deepcopy(baseline_cfg),\n",
    "                \"ckpt_path\": baseline_ckpt,\n",
    "                \"test_metrics\": metrics_b,\n",
    "            }\n",
    "            print(f\"[COMPARE] Baseline test accuracy: {metrics_b['accuracy']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Baseline evaluation failed: {e}\")\n",
    "    else:\n",
    "        print(f\"[INFO] Baseline checkpoint not found: {baseline_ckpt}\")\n",
    "\n",
    "    # ---- Greedy best ----\n",
    "    greedy_json = _safe_load_json(\"logs/greedy_results.json\")\n",
    "    if greedy_json and \"best_config\" in greedy_json:\n",
    "        g_cfg = greedy_json[\"best_config\"]\n",
    "        g_ckpt = _find_trial_ckpt(greedy_json, g_cfg)\n",
    "        if g_ckpt and os.path.exists(g_ckpt):\n",
    "            try:\n",
    "                test_loader = _build_test_loader(int(g_cfg.get(\"batch_size\", 32)))\n",
    "                model_g = _load_model_from_ckpt(g_cfg, g_ckpt)\n",
    "                metrics_g = _evaluate_on_test(model_g, test_loader)\n",
    "                out[\"greedy_best\"] = {\n",
    "                    \"config\": deepcopy(g_cfg),\n",
    "                    \"ckpt_path\": g_ckpt,\n",
    "                    \"test_metrics\": metrics_g,\n",
    "                }\n",
    "                print(f\"[COMPARE] Greedy-best test accuracy: {metrics_g['accuracy']:.4f} | cfg={g_cfg}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Greedy-best evaluation failed: {e}\")\n",
    "        else:\n",
    "            print(\"[INFO] Greedy-best checkpoint not found or missing exact match.\")\n",
    "    else:\n",
    "        print(\"[INFO] Greedy results JSON not found or missing best_config.\")\n",
    "\n",
    "    # ---- Optuna best ----\n",
    "    optuna_json = _safe_load_json(\"logs/optuna_results.json\")\n",
    "    if optuna_json and \"best_config\" in optuna_json:\n",
    "        o_cfg = optuna_json[\"best_config\"]\n",
    "        # Ensure full schema with defaults in case sampler omitted some keys\n",
    "        for k, v in {\"weight_decay\": 0.0, \"s\": 30.0, \"label_smoothing\": 0.0, \"batch_size\": 32}.items():\n",
    "            o_cfg.setdefault(k, v)\n",
    "        o_ckpt = _find_trial_ckpt(optuna_json, o_cfg)\n",
    "        if o_ckpt and os.path.exists(o_ckpt):\n",
    "            try:\n",
    "                test_loader = _build_test_loader(int(o_cfg.get(\"batch_size\", 32)))\n",
    "                model_o = _load_model_from_ckpt(o_cfg, o_ckpt)\n",
    "                metrics_o = _evaluate_on_test(model_o, test_loader)\n",
    "                out[\"optuna_best\"] = {\n",
    "                    \"config\": deepcopy(o_cfg),\n",
    "                    \"ckpt_path\": o_ckpt,\n",
    "                    \"test_metrics\": metrics_o,\n",
    "                }\n",
    "                print(f\"[COMPARE] Optuna-best test accuracy: {metrics_o['accuracy']:.4f} | cfg={o_cfg}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Optuna-best evaluation failed: {e}\")\n",
    "        else:\n",
    "            print(\"[INFO] Optuna-best checkpoint not found or missing exact match.\")\n",
    "    else:\n",
    "        print(\"[INFO] Optuna results JSON not found or missing best_config.\")\n",
    "\n",
    "    # ---- Write comparison file ----\n",
    "    with open(\"logs/final_compare.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out, f, ensure_ascii=False, indent=2)\n",
    "    print('[COMPARE] Written \"logs/final_compare.json\".')\n",
    "\n",
    "# Entry point switch\n",
    "RUN_FINAL_COMPARE = True  # set to False to skip the final comparison\n",
    "if RUN_FINAL_COMPARE:\n",
    "    compare_best_and_baseline()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SC4001_GroupAssignment (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
