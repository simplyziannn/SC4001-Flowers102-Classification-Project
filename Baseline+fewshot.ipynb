{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "264857e8",
   "metadata": {},
   "source": [
    "## Flowers102: baseline and few-shot pipeline\n",
    "This document contains the code implementation for sections 2.1-2.2 and 3.3-3.4 of the report, and includes the following:\n",
    " - **Baseline classification**\n",
    "   - ResNet-50 backbone with a cosine classifier head.\n",
    "   - Supervised training on the train split with early stopping on validation loss.\n",
    "   - Final evaluation on the test split with accuracy and macro/weighted metrics.\n",
    " - **Few-shot (Siamese + contrastive)**\n",
    "   - Shared ResNet-50 backbone and projection head.\n",
    "   - K-shot subset construction and balanced positive/negative pair sampling.\n",
    "   - Contrastive loss training and N-way K-shot episodic evaluation.\n",
    " - **Few-shot (Triplet)**\n",
    "   - TripletNet with the same backbone interface and a projection head.\n",
    "   - Triplet (anchor, positive, negative) sampling from the K-shot subset.\n",
    "   - TripletMarginLoss training and the same episodic evaluation protocol. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05161f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, UnidentifiedImageError, ImageFile\n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5e19aa",
   "metadata": {},
   "source": [
    "# Baseline Flowers102 pipeline setup\n",
    " - Initialize the baseline Flowers102 classification experiment using shared utilities from `flowers_common`.\n",
    " - Import helpers for seeding, device selection, data loading, model construction, training, evaluation, and inference.\n",
    " - Fix random seeds to ensure deterministic behavior across Python, NumPy, and PyTorch.\n",
    " - Select the compute device (CPU, CUDA, or MPS) via `get_device_config`.\n",
    " - Build deterministic train/validation/test `DataLoader` objects for the Flowers102 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37a9df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowers_common import seed_all, get_device_config, get_dataloaders\n",
    "from flowers_common import build_resnet50_cosine, safe_load_backbone_and_transplant_head\n",
    "from flowers_common import TrainConfig, train_model, evaluate_model, predict_image\n",
    "\n",
    "seed_all(1029)\n",
    "dc = get_device_config()\n",
    "device = dc.device\n",
    "train_loader, val_loader, test_loader = get_dataloaders(root=\"data\", batch_size=32, img_size=224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5646ebe5",
   "metadata": {},
   "source": [
    "## 2.1.1 Data and Batch Inspection (Consistent with Baseline Script)\n",
    "Print data partitioning scale, batch size, number of batches per round, and to provide a visual inspection of the shape of a batch, ensuring consistency with the original script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eae3d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "  Train: 1,020 samples\n",
      "  Val:   1,020 samples\n",
      "  Test:  6,149 samples\n",
      "\n",
      "Batches per epoch:\n",
      "  Train: 32\n",
      "  Val:   16\n",
      "  Test:  97\n",
      "\n",
      "Sample batch: torch.Size([32, 3, 224, 224]) images, torch.Size([32]) labels\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set, test_set = train_loader.dataset, val_loader.dataset, test_loader.dataset\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"  Train: {len(train_set):,} samples\")\n",
    "print(f\"  Val:   {len(val_set):,} samples\")\n",
    "print(f\"  Test:  {len(test_set):,} samples\")\n",
    "\n",
    "def num_batches(loader): return len(loader)\n",
    "print(\"\\nBatches per epoch:\")\n",
    "print(f\"  Train: {num_batches(train_loader)}\")\n",
    "print(f\"  Val:   {num_batches(val_loader)}\")\n",
    "print(f\"  Test:  {num_batches(test_loader)}\")\n",
    "\n",
    "imgs, labels = next(iter(train_loader))\n",
    "print(f\"\\nSample batch: {imgs.shape} images, {labels.shape} labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2dbb34",
   "metadata": {},
   "source": [
    "## 2.1.2 Build the model and load existing weights as needed.\n",
    "\n",
    "Use the ResNet50+Cosine head; if old weights exist, load the backbone in non-strict mode and attempt to migrate `fc` weights to the cosine head.\n",
    "- Old weights: `ckpt/resnet50_imagenet_finetuned_v1.pth`\n",
    "- New save: See subsequent training units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db468ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] No existing checkpoint loaded or file missing.\n"
     ]
    }
   ],
   "source": [
    "LOAD_WEIGHTS = True\n",
    "CKPT_EXISTING = \"ckpt/resnet50_imagenet_finetuned_v1.pth\"\n",
    "\n",
    "model = build_resnet50_cosine(num_classes=102, pretrained=True, device=device)\n",
    "if LOAD_WEIGHTS and os.path.exists(CKPT_EXISTING):\n",
    "    safe_load_backbone_and_transplant_head(model, CKPT_EXISTING, device=device, cosine_scale=30.0)\n",
    "else:\n",
    "    print(\"[INFO] No existing checkpoint loaded or file missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe4c92b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programs\\SC4001_GroupAssignment\\flowers_common.py:289: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=device_cfg.amp_enabled)\n",
      "c:\\Programs\\SC4001_GroupAssignment\\flowers_common.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "c:\\Programs\\SC4001_GroupAssignment\\flowers_common.py:333: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train 4.5387/0.0451 | Val 4.1008/0.1206\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 02 | Train 3.5230/0.3471 | Val 3.2684/0.4059\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 03 | Train 2.5164/0.6716 | Val 2.4644/0.6353\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 04 | Train 1.6920/0.8480 | Val 1.8150/0.7569\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 05 | Train 1.0726/0.9167 | Val 1.3315/0.8235\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 06 | Train 0.6998/0.9608 | Val 1.0517/0.8490\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 07 | Train 0.4246/0.9863 | Val 0.8656/0.8578\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 08 | Train 0.2806/0.9902 | Val 0.7203/0.8824\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 09 | Train 0.1974/0.9902 | Val 0.6433/0.8824\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 10 | Train 0.1318/0.9971 | Val 0.5819/0.8833\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 11 | Train 0.1103/0.9980 | Val 0.5508/0.8922\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 12 | Train 0.0837/0.9971 | Val 0.4948/0.9029\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 13 | Train 0.0620/1.0000 | Val 0.4780/0.8990\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 14 | Train 0.0550/0.9980 | Val 0.4734/0.8912\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 15 | Train 0.0540/0.9980 | Val 0.4677/0.8922\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 16 | Train 0.0476/0.9971 | Val 0.4492/0.9020\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 17 | Train 0.0321/1.0000 | Val 0.4271/0.9069\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 18 | Train 0.0273/1.0000 | Val 0.4094/0.9039\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 19 | Train 0.0251/1.0000 | Val 0.4047/0.9059\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 20 | Train 0.0237/0.9990 | Val 0.4048/0.9059\n",
      " EarlyStopping counter 1/5\n",
      "Epoch 21 | Train 0.0193/1.0000 | Val 0.4000/0.9088\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 22 | Train 0.0174/1.0000 | Val 0.3870/0.9059\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 23 | Train 0.0188/0.9990 | Val 0.3836/0.9088\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 24 | Train 0.0146/1.0000 | Val 0.3633/0.9137\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 25 | Train 0.0122/1.0000 | Val 0.3646/0.9127\n",
      " EarlyStopping counter 1/5\n",
      "Epoch 26 | Train 0.0134/0.9990 | Val 0.3630/0.9049\n",
      "Saved best model → ckpt/best_cosine_auto.pth\n",
      "Epoch 27 | Train 0.0236/0.9980 | Val 0.3971/0.9029\n",
      " EarlyStopping counter 1/5\n",
      "Epoch 28 | Train 0.0130/1.0000 | Val 0.3994/0.9020\n",
      " EarlyStopping counter 2/5\n",
      "Epoch 29 | Train 0.0127/1.0000 | Val 0.3711/0.9108\n",
      " EarlyStopping counter 3/5\n",
      "Epoch 30 | Train 0.0101/1.0000 | Val 0.3883/0.9029\n",
      " EarlyStopping counter 4/5\n",
      "{'best_epoch': 26, 'best_val_loss': 0.36299544107168913, 'best_val_acc': 0.9049019607843137, 'ckpt_path': 'ckpt/best_cosine_auto.pth'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "TRAIN = True\n",
    "CKPT_NEW = \"ckpt/best_cosine_auto.pth\"\n",
    "cfg = TrainConfig(epochs=30, lr=1e-4, patience=5, ckpt_path=CKPT_NEW)\n",
    "\n",
    "if TRAIN:\n",
    "    out = train_model(model, train_loader, val_loader, cfg, device_cfg=dc, epoch_log_cb=None)\n",
    "    print(out)\n",
    "else:\n",
    "    print(\"[SKIP] TRAIN=False; skipping baseline training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3e3f1",
   "metadata": {},
   "source": [
    "## 2.2.1 Training and Evaluation Without Data Augmentation\n",
    "\n",
    "Under the same hyperparameters as the baseline, retrain using the **training set without data augmentation**, and evaluate both models (with and without augmentation) on the **same test set**, comparing the differences in metrics.\n",
    "\n",
    "- Rebuild the DataLoader without data augmentation (`augment=False`).\n",
    "\n",
    "- Train using the same `TrainConfig` and workflow as the baseline, saving to `ckpt/best_cosine_noaug.pth`.\n",
    "\n",
    "- Load `ckpt/best_cosine_auto.pth` (with augmentation) and `ckpt/best_cosine_noaug.pth` (without augmentation) for test set evaluation.\n",
    "\n",
    "- Summarize accuracy and F1 scores and display a comparison table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "270e2f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train 4.5678/0.0461 | Val 3.9826/0.1853\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 02 | Train 2.5836/0.8186 | Val 3.1637/0.4863\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 03 | Train 1.2714/0.9892 | Val 2.6063/0.6461\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 04 | Train 0.5743/1.0000 | Val 2.2516/0.7176\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 05 | Train 0.2670/1.0000 | Val 2.0266/0.7598\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 06 | Train 0.1469/1.0000 | Val 1.9039/0.7686\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 07 | Train 0.0963/1.0000 | Val 1.8224/0.7676\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 08 | Train 0.0723/1.0000 | Val 1.7594/0.7725\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 09 | Train 0.0576/1.0000 | Val 1.6923/0.7775\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 10 | Train 0.0444/1.0000 | Val 1.6477/0.7843\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 11 | Train 0.0374/1.0000 | Val 1.6083/0.7833\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 12 | Train 0.0311/1.0000 | Val 1.5801/0.7824\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 13 | Train 0.0291/1.0000 | Val 1.5423/0.7892\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 14 | Train 0.0243/1.0000 | Val 1.5122/0.7931\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 15 | Train 0.0213/1.0000 | Val 1.4782/0.7951\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 16 | Train 0.0182/1.0000 | Val 1.4689/0.7912\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 17 | Train 0.0169/1.0000 | Val 1.4469/0.7971\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 18 | Train 0.0152/1.0000 | Val 1.4172/0.8000\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 19 | Train 0.0143/1.0000 | Val 1.4107/0.7941\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 20 | Train 0.0116/1.0000 | Val 1.3918/0.7961\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 21 | Train 0.0114/1.0000 | Val 1.3816/0.8000\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 22 | Train 0.0113/1.0000 | Val 1.3673/0.7980\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 23 | Train 0.0098/1.0000 | Val 1.3440/0.7931\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 24 | Train 0.0091/1.0000 | Val 1.3324/0.7961\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 25 | Train 0.0084/1.0000 | Val 1.3137/0.8029\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 26 | Train 0.0081/1.0000 | Val 1.3093/0.8049\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 27 | Train 0.0070/1.0000 | Val 1.2940/0.8098\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 28 | Train 0.0067/1.0000 | Val 1.2891/0.8078\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 29 | Train 0.0066/1.0000 | Val 1.2820/0.7961\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "Epoch 30 | Train 0.0058/1.0000 | Val 1.2621/0.8020\n",
      "Saved best model → ckpt/best_cosine_noaug.pth\n",
      "{'best_epoch': 30, 'best_val_loss': 1.2621473856270313, 'best_val_acc': 0.8019607843137255, 'ckpt_path': 'ckpt/best_cosine_noaug.pth'}\n"
     ]
    }
   ],
   "source": [
    "train_loader_noaug, val_loader_noaug, test_loader_noaug = get_dataloaders(\n",
    "    root=\"data\", batch_size=32, img_size=224, augment=False\n",
    ")\n",
    "\n",
    "TRAIN_NOAUG = True\n",
    "CKPT_NOAUG = \"ckpt/best_cosine_noaug.pth\"\n",
    "\n",
    "try:\n",
    "    LOAD_WEIGHTS\n",
    "except NameError:\n",
    "    LOAD_WEIGHTS = True\n",
    "try:\n",
    "    CKPT_EXISTING\n",
    "except NameError:\n",
    "    CKPT_EXISTING = \"ckpt/resnet50_imagenet_finetuned_v1.pth\"\n",
    "\n",
    "model_noaug = build_resnet50_cosine(num_classes=102, pretrained=True, device=device)\n",
    "if LOAD_WEIGHTS and os.path.exists(CKPT_EXISTING):\n",
    "    safe_load_backbone_and_transplant_head(model_noaug, CKPT_EXISTING, device=device, cosine_scale=30.0)\n",
    "\n",
    "cfg_noaug = TrainConfig(epochs=30, lr=1e-4, patience=5, ckpt_path=CKPT_NOAUG)\n",
    "\n",
    "if TRAIN_NOAUG:\n",
    "    out_noaug = train_model(model_noaug, train_loader_noaug, val_loader_noaug, cfg_noaug, device_cfg=dc, epoch_log_cb=None)\n",
    "    print(out_noaug)\n",
    "else:\n",
    "    print(\"[SKIP] TRAIN_NOAUG=False; skipping no-augmentation training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77ab0f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programs\\SC4001_GroupAssignment\\flowers_common.py:406: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "c:\\Programs\\SC4001_GroupAssignment\\flowers_common.py:406: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Setting  Accuracy  Macro F1  Macro Precision  Macro Recall  Weighted F1  Weighted Precision  Weighted Recall\n",
      "With Aug  0.894129  0.893732         0.887430      0.912919     0.894941            0.907822         0.894129\n",
      "  No Aug  0.792812  0.778482         0.770266      0.806773     0.793224            0.813344         0.792812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programs\\SC4001_GroupAssignment\\flowers_common.py:406: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8876240039030737, 'macro_avg': {'precision': 0.8814031472960611, 'recall': 0.9070421440957819, 'f1': 0.8873378436877947}, 'weighted_avg': {'precision': 0.9009453265318921, 'recall': 0.8876240039030737, 'f1': 0.8875011026434374}}\n"
     ]
    }
   ],
   "source": [
    "def _load_model_from_ckpt(ckpt_path: str):\n",
    "    m = build_resnet50_cosine(num_classes=102, pretrained=True, device=device)\n",
    "    if os.path.exists(ckpt_path):\n",
    "        state = torch.load(ckpt_path, map_location=device)\n",
    "        m.load_state_dict(state, strict=False)\n",
    "    else:\n",
    "        print(f\"[WARN] Checkpoint not found: {ckpt_path}\")\n",
    "    return m.eval()\n",
    "\n",
    "CKPT_AUG   = \"ckpt/best_cosine_auto.pth\"\n",
    "CKPT_NOAUG = \"ckpt/best_cosine_noaug.pth\"\n",
    "\n",
    "model_aug_best   = _load_model_from_ckpt(CKPT_AUG)\n",
    "model_noaug_best = _load_model_from_ckpt(CKPT_NOAUG)\n",
    "\n",
    "metrics_aug   = evaluate_model(model_aug_best, test_loader, device=device, amp_enabled=dc.amp_enabled)\n",
    "metrics_noaug = evaluate_model(model_noaug_best, test_loader, device=device, amp_enabled=dc.amp_enabled)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def _flat(name, m):\n",
    "    return {\n",
    "        \"Setting\": name,\n",
    "        \"Accuracy\": m[\"accuracy\"],\n",
    "        \"Macro F1\": m[\"macro_avg\"][\"f1\"],\n",
    "        \"Macro Precision\": m[\"macro_avg\"][\"precision\"],\n",
    "        \"Macro Recall\": m[\"macro_avg\"][\"recall\"],\n",
    "        \"Weighted F1\": m[\"weighted_avg\"][\"f1\"],\n",
    "        \"Weighted Precision\": m[\"weighted_avg\"][\"precision\"],\n",
    "        \"Weighted Recall\": m[\"weighted_avg\"][\"recall\"],\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame([_flat(\"With Aug\", metrics_aug), _flat(\"No Aug\", metrics_noaug)])\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "metrics = evaluate_model(model, test_loader, device=device, amp_enabled=dc.amp_enabled)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b748e5",
   "metadata": {},
   "source": [
    "## 3.3.1: Siamese Few-Shot (pairs + episodic eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b50b23",
   "metadata": {},
   "source": [
    "This section tests how well our model can recognize flowers when given only a few examples per class, aka concept known as few-shot learning.\n",
    "\n",
    "Instead of training the model to directly predict flower labels, we train it to measure how similar two images are.\n",
    "It learns to bring similar flowers closer together in its feature space, and push different flowers further apart.\n",
    "\n",
    "We do this by:\n",
    "\n",
    "1. Creating image pairs, some from the same class (positive) and some from different classes (negative).\n",
    "\n",
    "2. Training a Siamese network that passes both images through the same ResNet-50 backbone and compares their feature distance.\n",
    "\n",
    "3. Using a contrastive loss, which teaches the network to minimize distance for similar pairs and maximize distance for different pairs.\n",
    "\n",
    "4. After training, we evaluate using few-shot episodes: the model sees only a few “support” images per class and classifies new “query” images by comparing how close they are to each class’s examples.\n",
    "\n",
    "In short, this section helps the model learn similarity instead of strict classification, allowing it to generalize better when only a few training images are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "333fcfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FewShot] Using device: cuda:0\n",
      "[FewShot] K-shot chosen per class: 5\n",
      "[FewShot] Pair dataset size: 5,100 (pos≈neg)\n",
      "Siamese(\n",
      "  (backbone): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (proj): ProjectionHead(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Siamese] Epoch 1/10: 100%|██████████| 80/80 [01:59<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train contrastive loss: 0.0371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Siamese] Epoch 2/10: 100%|██████████| 80/80 [02:01<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train contrastive loss: 0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Siamese] Epoch 3/10: 100%|██████████| 80/80 [01:40<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train contrastive loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Siamese] Epoch 4/10: 100%|██████████| 80/80 [01:37<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train contrastive loss: 0.0101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Siamese] Epoch 5/10: 100%|██████████| 80/80 [01:27<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train contrastive loss: 0.0085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Siamese] Epoch 6/10: 100%|██████████| 80/80 [01:26<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train contrastive loss: 0.0075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Siamese] Epoch 7/10: 100%|██████████| 80/80 [01:26<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train contrastive loss: 0.0067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Siamese] Epoch 8/10: 100%|██████████| 80/80 [01:26<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train contrastive loss: 0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Siamese] Epoch 9/10: 100%|██████████| 80/80 [01:26<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train contrastive loss: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Siamese] Epoch 10/10: 100%|██████████| 80/80 [01:26<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train contrastive loss: 0.0054\n",
      "[FewShot] Saved projection head: ckpt/siamese_proj.pth\n",
      "[FewShot] 5-way 1-shot accuracy on VAL: 97.00% ± 0.33% (s.e.)\n",
      "[FewShot] 5-way 5-shot accuracy on VAL: 97.76% ± 0.22% (s.e.)\n",
      "[Saved] Results written to results/fewshot_siamese_results.json\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3.3.1: Siamese Few-Shot (Contrastive) \n",
    "# =============================================================================\n",
    "\n",
    "# ---- Config (few-shot) ----\n",
    "FEWSHOT_K          = 5            # K images per class for training (K-shot)\n",
    "PAIRS_PER_CLASS    = 40           # how many positive pairs per class to prebuild (negatives are matched to keep balance)\n",
    "MARGIN             = 1.0          # margin for contrastive loss\n",
    "EPOCHS_SIAMESE     = 10\n",
    "LR_SIAMESE         = 3e-4\n",
    "BATCH_SIZE_PAIRS   = 64\n",
    "EPISODES           = 200          # N-way K-shot evaluation episodes\n",
    "N_WAY              = 5            # number of classes per episode\n",
    "Q_PER_CLASS        = 5            # queries per class in episodic eval\n",
    "SEED               = 1029\n",
    "\n",
    "rng = random.Random(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Use the same device you set earlier (Section 4 picked CUDA or MPS)\n",
    "print(f\"[FewShot] Using device: {device}\")\n",
    "\n",
    "# ---------- Helpers: take a K-shot subset of the Flowers102 train split ----------\n",
    "def build_class_index(dataset):\n",
    "    by_cls = defaultdict(list)\n",
    "    for idx, (_, y) in enumerate(dataset):\n",
    "        by_cls[int(y)].append(idx)\n",
    "    return by_cls\n",
    "\n",
    "def kshot_indices(by_cls, k, rng):\n",
    "    subset = []\n",
    "    for c, idxs in by_cls.items():\n",
    "        if len(idxs) < k:\n",
    "            # In Flowers102, train has ≥ 10 per class; still guard:\n",
    "            chosen = idxs if len(idxs) == k else rng.sample(idxs, min(k, len(idxs)))\n",
    "        else:\n",
    "            chosen = rng.sample(idxs, k)\n",
    "        subset.extend(chosen)\n",
    "    return sorted(subset)\n",
    "\n",
    "# ---------- Pair list (precomputed for determinism) ----------\n",
    "def build_pairs(index_by_class, k_per_class, pairs_per_class, rng):\n",
    "    # choose k per class deterministically\n",
    "    chosen_per_class = {c: rng.sample(idxs, min(k_per_class, len(idxs)))\n",
    "                        for c, idxs in index_by_class.items()}\n",
    "\n",
    "    pos_pairs = []\n",
    "    for c, idxs in chosen_per_class.items():\n",
    "        # all unordered positive pairs\n",
    "        cand = []\n",
    "        for i in range(len(idxs)):\n",
    "            for j in range(i+1, len(idxs)):\n",
    "                cand.append( (idxs[i], idxs[j]) )\n",
    "        rng.shuffle(cand)\n",
    "        pos_pairs.extend([(a,b,1) for (a,b) in cand[:pairs_per_class]])\n",
    "\n",
    "    # negatives: for each class, sample pairs with a different class\n",
    "    all_classes = list(chosen_per_class.keys())\n",
    "    neg_pairs = []\n",
    "    for c in all_classes:\n",
    "        a_list = rng.choices(chosen_per_class[c], k=pairs_per_class)\n",
    "        for a in a_list:\n",
    "            c2 = rng.choice([x for x in all_classes if x != c])\n",
    "            b = rng.choice(chosen_per_class[c2])\n",
    "            neg_pairs.append((a,b,0))\n",
    "\n",
    "    pairs = pos_pairs + neg_pairs\n",
    "    rng.shuffle(pairs)\n",
    "    return pairs, chosen_per_class\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, base_dataset, pairs):\n",
    "        self.base = base_dataset\n",
    "        self.pairs = pairs\n",
    "        self.tf = base_dataset.transform  # use same transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        i1, i2, y = self.pairs[i]\n",
    "        x1, _ = self.base[i1]\n",
    "        x2, _ = self.base[i2]\n",
    "        return x1, x2, torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# ---------- Siamese model: shared backbone + projection ----------\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim=2048, out_dim=512):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.net(x)\n",
    "        return F.normalize(z, dim=1)\n",
    "\n",
    "class Siamese(nn.Module):\n",
    "    def __init__(self, backbone, proj_out=512, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone           # expects [B,3,224,224] -> [B,2048]\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.proj = ProjectionHead(2048, proj_out)\n",
    "\n",
    "    def embed(self, x):\n",
    "        feats = self.backbone(x)           # [B,2048]\n",
    "        return self.proj(feats)            # [B,proj_out] (L2-normalized)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        z1 = self.embed(x1)\n",
    "        z2 = self.embed(x2)\n",
    "        # cosine similarity for monitoring (not used in loss directly)\n",
    "        sim = (z1 * z2).sum(dim=1)\n",
    "        return z1, z2, sim\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Hadsell et al. 2006\n",
    "    y=1 for positive (same class), y=0 for negative.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, z1, z2, y):\n",
    "        d = F.pairwise_distance(z1, z2)            # [B]\n",
    "        pos = y * (d ** 2)\n",
    "        neg = (1 - y) * F.relu(self.margin - d) ** 2\n",
    "        return (pos + neg).mean()\n",
    "\n",
    "# ---------- Build pairs from your existing train_set ----------\n",
    "train_set_fs = train_loader.dataset     # Flowers102 split=\"train\"\n",
    "val_set_fs   = val_loader.dataset\n",
    "\n",
    "by_cls = build_class_index(train_set_fs)\n",
    "pairs, chosen_per_class = build_pairs(by_cls, FEWSHOT_K, PAIRS_PER_CLASS, rng)\n",
    "pair_ds = PairDataset(train_set_fs, pairs)\n",
    "pair_loader = DataLoader(pair_ds, batch_size=BATCH_SIZE_PAIRS,\n",
    "                         shuffle=True, num_workers=0, pin_memory=False,\n",
    "                         generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "print(f\"[FewShot] K-shot chosen per class: {FEWSHOT_K}\")\n",
    "print(f\"[FewShot] Pair dataset size: {len(pair_ds):,} (pos≈neg)\")\n",
    "\n",
    "# ---------- Siamese model from Section 3 backbone ----------\n",
    "# classifier 'model' is nn.Sequential([resnet50_backbone, CosineClassifier])\n",
    "backbone = model[0]                         # reuse trained backbone\n",
    "siam = Siamese(backbone, proj_out=512, freeze_backbone=True).to(device)\n",
    "criterion = ContrastiveLoss(MARGIN)\n",
    "opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, siam.parameters()), lr=LR_SIAMESE)\n",
    "\n",
    "print(siam)\n",
    "\n",
    "# ---------- Train (pairs) ----------\n",
    "for epoch in range(1, EPOCHS_SIAMESE + 1):\n",
    "    siam.train()\n",
    "    running, n = 0.0, 0\n",
    "    for x1, x2, y in tqdm(pair_loader, desc=f\"[Siamese] Epoch {epoch}/{EPOCHS_SIAMESE}\"):\n",
    "        x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        z1, z2, _ = siam(x1, x2)\n",
    "        loss = criterion(z1, z2, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running += loss.item() * x1.size(0)\n",
    "        n += x1.size(0)\n",
    "    print(f\"  Train contrastive loss: {running/n:.4f}\")\n",
    "\n",
    "# --------- Save siamese projection (optional) ----------\n",
    "os.makedirs(\"ckpt\", exist_ok=True)\n",
    "torch.save({\"proj\": siam.proj.state_dict()}, \"ckpt/siamese_proj.pth\")\n",
    "print(\"[FewShot] Saved projection head: ckpt/siamese_proj.pth\")\n",
    "\n",
    "# ---------- Episodic N-way K-shot evaluation (prototype nearest) ----------\n",
    "@torch.no_grad()\n",
    "def episodic_eval(backbone, proj, base_dataset, n_way=5, k_shot=1, q_per_class=5, episodes=200):\n",
    "    backbone.eval()\n",
    "    proj.eval()\n",
    "    by_cls = build_class_index(base_dataset)\n",
    "    classes = list(by_cls.keys())\n",
    "    accs = []\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        episode_classes = rng.sample(classes, n_way)\n",
    "\n",
    "        # support / query split\n",
    "        support_idx = []\n",
    "        query_idx = []\n",
    "        for c in episode_classes:\n",
    "            idxs = by_cls[c]\n",
    "            if len(idxs) < k_shot + q_per_class:\n",
    "                # fallback: sample with replacement if needed\n",
    "                sampled = rng.choices(idxs, k=k_shot + q_per_class)\n",
    "            else:\n",
    "                sampled = rng.sample(idxs, k=k_shot + q_per_class)\n",
    "            support_idx.extend([(i, c) for i in sampled[:k_shot]])\n",
    "            query_idx.extend([(i, c) for i in sampled[k_shot:]])\n",
    "\n",
    "        # build tensors\n",
    "        def load_batch(pairs):\n",
    "            xs, ys = [], []\n",
    "            for i, c in pairs:\n",
    "                x, _ = base_dataset[i]\n",
    "                xs.append(x)\n",
    "                ys.append(c)\n",
    "            xs = torch.stack(xs).to(device)\n",
    "            ys = torch.tensor(ys, device=device)\n",
    "            return xs, ys\n",
    "\n",
    "        xs_s, ys_s = load_batch(support_idx)\n",
    "        xs_q, ys_q = load_batch(query_idx)\n",
    "\n",
    "        # embeddings\n",
    "        z_s = F.normalize(proj(backbone(xs_s)), dim=1)   # [n_way*k, d]\n",
    "        z_q = F.normalize(proj(backbone(xs_q)), dim=1)   # [n_way*q, d]\n",
    "\n",
    "        # class prototypes\n",
    "        protos = []\n",
    "        for c in episode_classes:\n",
    "            zc = z_s[ (ys_s == c) ]\n",
    "            protos.append(zc.mean(dim=0))\n",
    "        protos = torch.stack(protos)                     # [n_way, d]\n",
    "\n",
    "        # cosine similarity to prototypes\n",
    "        sims = z_q @ protos.t()                          # [n_way*q, n_way]\n",
    "        preds = sims.argmax(dim=1)\n",
    "\n",
    "        # map true labels to 0..n_way-1\n",
    "        class_to_pos = {c:i for i,c in enumerate(episode_classes)}\n",
    "        ys_true = torch.tensor([class_to_pos[int(c)] for c in ys_q.tolist()], device=device)\n",
    "\n",
    "        acc = (preds == ys_true).float().mean().item()\n",
    "        accs.append(acc)\n",
    "\n",
    "    return float(np.mean(accs)), float(np.std(accs)/math.sqrt(len(accs)))\n",
    "\n",
    "mean_acc, se = episodic_eval(backbone=siam.backbone, proj=siam.proj,\n",
    "                             base_dataset=val_loader.dataset,\n",
    "                             n_way=N_WAY, k_shot=1, q_per_class=Q_PER_CLASS,\n",
    "                             episodes=EPISODES)\n",
    "print(f\"[FewShot] {N_WAY}-way 1-shot accuracy on VAL: {mean_acc*100:.2f}% ± {se*100:.2f}% (s.e.)\")\n",
    "\n",
    "# You can also test 5-way 5-shot:\n",
    "mean_acc5, se5 = episodic_eval(backbone=siam.backbone, proj=siam.proj,\n",
    "                               base_dataset=val_loader.dataset,\n",
    "                               n_way=N_WAY, k_shot=5, q_per_class=Q_PER_CLASS,\n",
    "                               episodes=EPISODES)\n",
    "print(f\"[FewShot] {N_WAY}-way 5-shot accuracy on VAL: {mean_acc5*100:.2f}% ± {se5*100:.2f}% (s.e.)\")\n",
    "\n",
    "\n",
    "import json, time\n",
    "\n",
    "results_siamese = {\n",
    "    \"model\": \"Siamese (Contrastive)\",\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"dataset_split\": \"val\",\n",
    "    \"n_way\": N_WAY,\n",
    "    \"q_per_class\": Q_PER_CLASS,\n",
    "    \"1-shot_accuracy\": round(mean_acc * 100, 2),\n",
    "    \"1-shot_std_error\": round(se * 100, 2),\n",
    "    \"5-shot_accuracy\": round(mean_acc5 * 100, 2),\n",
    "    \"5-shot_std_error\": round(se5 * 100, 2),\n",
    "}\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/fewshot_siamese_results.json\", \"w\") as f:\n",
    "    json.dump(results_siamese, f, indent=4)\n",
    "\n",
    "print(f\"[Saved] Results written to results/fewshot_siamese_results.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87962cfa",
   "metadata": {},
   "source": [
    "## 3.3.2 Triplet Loss for Improved Few-Shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a60b0",
   "metadata": {},
   "source": [
    "This section builds on the previous Siamese setup but uses triplets of images instead of pairs.\n",
    "Each triplet consists of:\n",
    "\n",
    "- Anchor – a reference image,\n",
    "\n",
    "- Positive – another image of the same class, and\n",
    "\n",
    "- Negative – an image from a different class.\n",
    "\n",
    "The model learns by comparing the distances between these three embeddings:\n",
    "\n",
    "- It tries to make the anchor–positive distance smaller,\n",
    "\n",
    "- and the anchor–negative distance larger by at least a certain margin.\n",
    "\n",
    "This is called Triplet Loss, and it encourages the network to create a more structured embedding space where images of the same class form tight clusters and different classes are clearly separated.\n",
    "\n",
    "By training with triplets, the model can further improve its ability to distinguish visually similar flower types when only a few examples are available — making it a stronger few-shot learner overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd71c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Triplet] Using device: cuda:0\n",
      "[Triplet] K-shot per class: 5\n",
      "[Triplet] Triplet dataset size: 6,120\n",
      "TripletNet(\n",
      "  (backbone): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (proj): ProjectionHead(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Triplet] Epoch 1/12: 100%|██████████| 96/96 [02:37<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train triplet loss: 0.0273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Triplet] Epoch 2/12: 100%|██████████| 96/96 [02:35<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train triplet loss: 0.0027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Triplet] Epoch 3/12: 100%|██████████| 96/96 [02:44<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train triplet loss: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Triplet] Epoch 4/12: 100%|██████████| 96/96 [02:43<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train triplet loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Triplet] Epoch 5/12: 100%|██████████| 96/96 [02:40<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train triplet loss: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Triplet] Epoch 6/12: 100%|██████████| 96/96 [02:42<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train triplet loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Triplet] Epoch 7/12: 100%|██████████| 96/96 [02:43<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train triplet loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Triplet] Epoch 8/12: 100%|██████████| 96/96 [02:39<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train triplet loss: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Triplet] Epoch 9/12: 100%|██████████| 96/96 [02:39<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train triplet loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Triplet] Epoch 10/12: 100%|██████████| 96/96 [02:39<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train triplet loss: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Triplet] Epoch 11/12: 100%|██████████| 96/96 [02:39<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train triplet loss: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Triplet] Epoch 12/12: 100%|██████████| 96/96 [02:39<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train triplet loss: 0.0005\n",
      "[Triplet] Saved projection head: ckpt/triplet_proj.pth\n",
      "[Triplet] 5-way 1-shot: 94.82% ± 0.46%\n",
      "[Triplet] 5-way 5-shot: 97.96% ± 0.21%\n",
      "[Saved] Results written to results/fewshot_triplet_results.json\n"
     ]
    }
   ],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  # Tolerate slightly truncated JPEGs\n",
    "\n",
    "# ---- Triplet configuration ----\n",
    "TRI_FEWSHOT_K        = 5\n",
    "TRIPLETS_PER_CLASS   = 60\n",
    "TRI_MARGIN           = 0.8\n",
    "TRI_EPOCHS           = 12\n",
    "TRI_LR               = 3e-4\n",
    "TRI_BATCH_SIZE       = 64\n",
    "TRI_EPISODES         = 200\n",
    "TRI_N_WAY            = 5\n",
    "TRI_Q_PER_CLASS      = 5\n",
    "TRI_SEED             = 1029\n",
    "\n",
    "rng = random.Random(TRI_SEED)\n",
    "np.random.seed(TRI_SEED)\n",
    "torch.manual_seed(TRI_SEED)\n",
    "\n",
    "print(f\"[Triplet] Using device: {device}\")\n",
    "\n",
    "# --- Reuse helpers from Section 7: build_class_index, episodic_eval, ProjectionHead ---\n",
    "\n",
    "def _dummy_tensor(base_dataset):\n",
    "    \"\"\"Create a placeholder image run through the SAME transform as the dataset.\"\"\"\n",
    "    pil = Image.new(\"RGB\", (256, 256), (0, 0, 0))\n",
    "    tf = getattr(base_dataset, \"transform\", None)\n",
    "    return tf(pil) if tf is not None else T.ToTensor()(pil)\n",
    "\n",
    "# --- Build triplets from existing Flowers102 train split ---\n",
    "def build_triplets(index_by_class, k_per_class, triplets_per_class, rng):\n",
    "    \"\"\"Returns (anchor, positive, negative) index triplets for each class.\"\"\"\n",
    "    chosen_per_class = {\n",
    "        c: (rng.sample(idxs, k_per_class) if len(idxs) >= k_per_class else rng.choices(idxs, k=k_per_class))\n",
    "        for c, idxs in index_by_class.items()\n",
    "    }\n",
    "\n",
    "    triplets = []\n",
    "    all_classes = list(index_by_class.keys())\n",
    "    for c in all_classes:\n",
    "        pool = chosen_per_class[c]\n",
    "        for _ in range(triplets_per_class):\n",
    "            a, p = rng.sample(pool, 2) if len(pool) >= 2 else (pool[0], pool[0])\n",
    "            neg_c = rng.choice([x for x in all_classes if x != c])\n",
    "            n = rng.choice(chosen_per_class[neg_c])\n",
    "            triplets.append((a, p, n))\n",
    "    rng.shuffle(triplets)\n",
    "    return triplets, chosen_per_class\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, base_dataset, triplets):\n",
    "        self.base = base_dataset\n",
    "        self.triplets = triplets\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "    def __getitem__(self, i):\n",
    "        a, p, n = self.triplets[i]\n",
    "        # Each access may raise UnidentifiedImageError if the JPEG is corrupted.\n",
    "        try:\n",
    "            xa, _ = self.base[a]\n",
    "        except (UnidentifiedImageError, OSError):\n",
    "            xa = _dummy_tensor(self.base)\n",
    "        try:\n",
    "            xp, _ = self.base[p]\n",
    "        except (UnidentifiedImageError, OSError):\n",
    "            xp = _dummy_tensor(self.base)\n",
    "        try:\n",
    "            xn, _ = self.base[n]\n",
    "        except (UnidentifiedImageError, OSError):\n",
    "            xn = _dummy_tensor(self.base)\n",
    "        return xa, xp, xn\n",
    "\n",
    "# --- Form triplets ---\n",
    "train_set_fs = train_loader.dataset\n",
    "val_set_fs   = val_loader.dataset\n",
    "index_by_class = build_class_index(train_set_fs)\n",
    "triplets, _ = build_triplets(index_by_class, TRI_FEWSHOT_K, TRIPLETS_PER_CLASS, rng)\n",
    "\n",
    "tri_ds = TripletDataset(train_set_fs, triplets)\n",
    "tri_loader = DataLoader(\n",
    "    tri_ds,\n",
    "    batch_size=TRI_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    generator=torch.Generator().manual_seed(TRI_SEED),\n",
    ")\n",
    "\n",
    "print(f\"[Triplet] K-shot per class: {TRI_FEWSHOT_K}\")\n",
    "print(f\"[Triplet] Triplet dataset size: {len(tri_ds):,}\")\n",
    "\n",
    "# --- Model + optimizer ---\n",
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, backbone, proj_out=512, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.proj = ProjectionHead(2048, proj_out)\n",
    "    def embed(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        return self.proj(feats)\n",
    "\n",
    "backbone = model[0]\n",
    "tri_model = TripletNet(backbone, proj_out=512, freeze_backbone=True).to(device)\n",
    "\n",
    "criterion = nn.TripletMarginLoss(margin=TRI_MARGIN, p=2)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, tri_model.parameters()), lr=TRI_LR)\n",
    "\n",
    "print(tri_model)\n",
    "\n",
    "# --- Training loop ---\n",
    "for epoch in range(1, TRI_EPOCHS + 1):\n",
    "    tri_model.train()\n",
    "    total_loss, count = 0.0, 0\n",
    "    for xa, xp, xn in tqdm(tri_loader, desc=f\"[Triplet] Epoch {epoch}/{TRI_EPOCHS}\"):\n",
    "        xa, xp, xn = xa.to(device), xp.to(device), xn.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        za = F.normalize(tri_model.embed(xa), dim=1)\n",
    "        zp = F.normalize(tri_model.embed(xp), dim=1)\n",
    "        zn = F.normalize(tri_model.embed(xn), dim=1)\n",
    "        loss = criterion(za, zp, zn)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xa.size(0)\n",
    "        count += xa.size(0)\n",
    "    print(f\"  Train triplet loss: {total_loss / count:.4f}\")\n",
    "\n",
    "# --- Save projection head ---\n",
    "os.makedirs(\"ckpt\", exist_ok=True)\n",
    "torch.save({\"proj\": tri_model.proj.state_dict()}, \"ckpt/triplet_proj.pth\")\n",
    "print(\"[Triplet] Saved projection head: ckpt/triplet_proj.pth\")\n",
    "\n",
    "# --- Episodic evaluation (reuse from 3.3.1: Siamese Few-Shot (Contrastive) ) ---\n",
    "mean1, se1 = episodic_eval(\n",
    "    backbone=tri_model.backbone,\n",
    "    proj=tri_model.proj,\n",
    "    base_dataset=val_set_fs,\n",
    "    n_way=TRI_N_WAY, k_shot=1, q_per_class=TRI_Q_PER_CLASS, episodes=TRI_EPISODES\n",
    ")\n",
    "print(f\"[Triplet] {TRI_N_WAY}-way 1-shot: {mean1*100:.2f}% ± {se1*100:.2f}%\")\n",
    "\n",
    "mean5, se5 = episodic_eval(\n",
    "    backbone=tri_model.backbone,\n",
    "    proj=tri_model.proj,\n",
    "    base_dataset=val_set_fs,\n",
    "    n_way=TRI_N_WAY, k_shot=5, q_per_class=TRI_Q_PER_CLASS, episodes=TRI_EPISODES\n",
    ")\n",
    "print(f\"[Triplet] {TRI_N_WAY}-way 5-shot: {mean5*100:.2f}% ± {se5*100:.2f}%\")\n",
    "\n",
    "import json, time\n",
    "\n",
    "results_triplet = {\n",
    "    \"model\": \"Triplet Loss\",\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"dataset_split\": \"val\",\n",
    "    \"n_way\": TRI_N_WAY,\n",
    "    \"q_per_class\": TRI_Q_PER_CLASS,\n",
    "    \"1-shot_accuracy\": round(mean1 * 100, 2),\n",
    "    \"1-shot_std_error\": round(se1 * 100, 2),\n",
    "    \"5-shot_accuracy\": round(mean5 * 100, 2),\n",
    "    \"5-shot_std_error\": round(se5 * 100, 2),\n",
    "}\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/fewshot_triplet_results.json\", \"w\") as f:\n",
    "    json.dump(results_triplet, f, indent=4)\n",
    "\n",
    "print(f\"[Saved] Results written to results/fewshot_triplet_results.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e14287c",
   "metadata": {},
   "source": [
    "## 3.4.1 Few-Shot Evaluation on TEST set (Siamese + Triplet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60147f7",
   "metadata": {},
   "source": [
    "**Purpose**\n",
    " - Produce final N-way K-shot results on the held-out TEST split for both metric-learning models.\n",
    " \n",
    "**Method**\n",
    " - Reuse the trained Siamese and Triplet models and the shared episodic evaluation function.\n",
    " - Run N-way 1-shot and 5-shot episodes on the TEST dataset for each model.\n",
    " - Log separate metrics for Siamese and Triplet configurations, including standard errors.\n",
    " - Aggregate all TEST metrics into a single dictionary and write them to a JSON file.\n",
    " - Use these outputs to compare baseline, Siamese, and Triplet few-shot performance in the report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "926dc28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running final few-shot evaluation on TEST split\n",
      "[Siamese-TEST] 5-way 1-shot: 96.70% ± 0.33%\n",
      "[Siamese-TEST] 5-way 5-shot: 98.12% ± 0.21%\n",
      "\n",
      "[Triplet-TEST] 5-way 1-shot: 96.78% ± 0.33%\n",
      "[Triplet-TEST] 5-way 5-shot: 98.38% ± 0.19%\n",
      "\n",
      "[INFO] Few-shot evaluation on TEST split completed.\n",
      "[Saved] Combined test results written to results/fewshot_test_results.json\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Running final few-shot evaluation on TEST split\")\n",
    "\n",
    "# --- Siamese evaluation on TEST set ---\n",
    "mean1_siam, se1_siam = episodic_eval(\n",
    "    backbone=siam.backbone,\n",
    "    proj=siam.proj,\n",
    "    base_dataset=test_loader.dataset,\n",
    "    n_way=N_WAY, k_shot=1, q_per_class=Q_PER_CLASS, episodes=EPISODES\n",
    ")\n",
    "mean5_siam, se5_siam = episodic_eval(\n",
    "    backbone=siam.backbone,\n",
    "    proj=siam.proj,\n",
    "    base_dataset=test_loader.dataset,\n",
    "    n_way=N_WAY, k_shot=5, q_per_class=Q_PER_CLASS, episodes=EPISODES\n",
    ")\n",
    "\n",
    "print(f\"[Siamese-TEST] {N_WAY}-way 1-shot: {mean1_siam*100:.2f}% ± {se1_siam*100:.2f}%\")\n",
    "print(f\"[Siamese-TEST] {N_WAY}-way 5-shot: {mean5_siam*100:.2f}% ± {se5_siam*100:.2f}%\\n\")\n",
    "\n",
    "# --- Triplet evaluation on TEST set ---\n",
    "mean1_triplet, se1_triplet = episodic_eval(\n",
    "    backbone=tri_model.backbone,\n",
    "    proj=tri_model.proj,\n",
    "    base_dataset=test_loader.dataset,\n",
    "    n_way=TRI_N_WAY, k_shot=1, q_per_class=TRI_Q_PER_CLASS, episodes=TRI_EPISODES\n",
    ")\n",
    "mean5_triplet, se5_triplet = episodic_eval(\n",
    "    backbone=tri_model.backbone,\n",
    "    proj=tri_model.proj,\n",
    "    base_dataset=test_loader.dataset,\n",
    "    n_way=TRI_N_WAY, k_shot=5, q_per_class=TRI_Q_PER_CLASS, episodes=TRI_EPISODES\n",
    ")\n",
    "\n",
    "print(f\"[Triplet-TEST] {TRI_N_WAY}-way 1-shot: {mean1_triplet*100:.2f}% ± {se1_triplet*100:.2f}%\")\n",
    "print(f\"[Triplet-TEST] {TRI_N_WAY}-way 5-shot: {mean5_triplet*100:.2f}% ± {se5_triplet*100:.2f}%\")\n",
    "\n",
    "print(\"\\n[INFO] Few-shot evaluation on TEST split completed.\")\n",
    "\n",
    "# --- Save combined TEST results ---\n",
    "combined_results = {\n",
    "    \"siamese_test\": {\n",
    "        \"1-shot_accuracy\": round(mean1_siam * 100, 2),\n",
    "        \"1-shot_std_error\": round(se1_siam * 100, 2),\n",
    "        \"5-shot_accuracy\": round(mean5_siam * 100, 2),\n",
    "        \"5-shot_std_error\": round(se5_siam * 100, 2),\n",
    "        \"episodes\": EPISODES,\n",
    "        \"n_way\": N_WAY,\n",
    "        \"q_per_class\": Q_PER_CLASS,\n",
    "    },\n",
    "    \"triplet_test\": {\n",
    "        \"1-shot_accuracy\": round(mean1_triplet * 100, 2),\n",
    "        \"1-shot_std_error\": round(se1_triplet * 100, 2),\n",
    "        \"5-shot_accuracy\": round(mean5_triplet * 100, 2),\n",
    "        \"5-shot_std_error\": round(se5_triplet * 100, 2),\n",
    "        \"episodes\": TRI_EPISODES,\n",
    "        \"n_way\": TRI_N_WAY,\n",
    "        \"q_per_class\": TRI_Q_PER_CLASS,\n",
    "    },\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/fewshot_test_results.json\", \"w\") as f:\n",
    "    json.dump(combined_results, f, indent=4)\n",
    "\n",
    "print(\"[Saved] Combined test results written to results/fewshot_test_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
